mm/access_process_vm: use the new follow_pfnmap API

jira LE-3557
Rebuild_History Non-Buildable kernel-5.14.0-570.26.1.el9_6
commit-author Peter Xu <peterx@redhat.com>
commit b17269a51cc7f046a6f2cf9a6c314a0de885e5a5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-570.26.1.el9_6/b17269a5.failed

Use the new API that can understand huge pfn mappings.

Link: https://lkml.kernel.org/r/20240826204353.2228736-16-peterx@redhat.com
	Signed-off-by: Peter Xu <peterx@redhat.com>
	Cc: Alexander Gordeev <agordeev@linux.ibm.com>
	Cc: Alex Williamson <alex.williamson@redhat.com>
	Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Christian Borntraeger <borntraeger@linux.ibm.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: Gavin Shan <gshan@redhat.com>
	Cc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>
	Cc: Heiko Carstens <hca@linux.ibm.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jason Gunthorpe <jgg@nvidia.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Niklas Schnelle <schnelle@linux.ibm.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Ryan Roberts <ryan.roberts@arm.com>
	Cc: Sean Christopherson <seanjc@google.com>
	Cc: Sven Schnelle <svens@linux.ibm.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vasily Gorbik <gor@linux.ibm.com>
	Cc: Will Deacon <will@kernel.org>
	Cc: Zi Yan <ziy@nvidia.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit b17269a51cc7f046a6f2cf9a6c314a0de885e5a5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index e2794e3b8919,cfc278691466..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -5749,37 -6341,34 +5749,45 @@@ int generic_access_phys(struct vm_area_
  	resource_size_t phys_addr;
  	unsigned long prot = 0;
  	void __iomem *maddr;
- 	pte_t *ptep, pte;
- 	spinlock_t *ptl;
  	int offset = offset_in_page(addr);
  	int ret = -EINVAL;
+ 	bool writable;
+ 	struct follow_pfnmap_args args = { .vma = vma, .address = addr };
  
 +	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
 +		return -EINVAL;
 +
  retry:
++<<<<<<< HEAD
 +	if (follow_pte(vma->vm_mm, addr, &ptep, &ptl))
++=======
+ 	if (follow_pfnmap_start(&args))
++>>>>>>> b17269a51cc7 (mm/access_process_vm: use the new follow_pfnmap API)
  		return -EINVAL;
- 	pte = ptep_get(ptep);
- 	pte_unmap_unlock(ptep, ptl);
+ 	prot = pgprot_val(args.pgprot);
+ 	phys_addr = (resource_size_t)args.pfn << PAGE_SHIFT;
+ 	writable = args.writable;
+ 	follow_pfnmap_end(&args);
  
- 	prot = pgprot_val(pte_pgprot(pte));
- 	phys_addr = (resource_size_t)pte_pfn(pte) << PAGE_SHIFT;
- 
- 	if ((write & FOLL_WRITE) && !pte_write(pte))
+ 	if ((write & FOLL_WRITE) && !writable)
  		return -EINVAL;
  
  	maddr = ioremap_prot(phys_addr, PAGE_ALIGN(len + offset), prot);
  	if (!maddr)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	if (follow_pte(vma->vm_mm, addr, &ptep, &ptl))
++=======
+ 	if (follow_pfnmap_start(&args))
++>>>>>>> b17269a51cc7 (mm/access_process_vm: use the new follow_pfnmap API)
  		goto out_unmap;
  
- 	if (!pte_same(pte, ptep_get(ptep))) {
- 		pte_unmap_unlock(ptep, ptl);
+ 	if ((prot != pgprot_val(args.pgprot)) ||
+ 	    (phys_addr != (args.pfn << PAGE_SHIFT)) ||
+ 	    (writable != args.writable)) {
+ 		follow_pfnmap_end(&args);
  		iounmap(maddr);
- 
  		goto retry;
  	}
  
* Unmerged path mm/memory.c
